				Zillow Clone Data Scraper and Google Form Submission
This project is a web scraping and data entry automation script that retrieves property listings from a Zillow clone website, processes the data, and automatically submits it to a custom Google Form. It leverages Python libraries such as BeautifulSoup for web scraping and Selenium for automated form submission.

						Table of Contents
. Project Overview
. Requirements
. Setup and Installation
. How It Works
. Usage
. Improvements and Future Work

						Project Overview
The main objective of this project is to:

Scrape property listings, including links, prices, and addresses, from a Zillow clone website.
Submit the collected data to a Google Form automatically, filling in each property as a separate form response.
The end result is a Google Sheet (generated by the form) that consolidates all the property data.
							Requirements
To run this project, you need:
. Python 3.x
. A web browser (Google Chrome) with an installed driver compatible with Selenium
. A Google Form with fields for:
. Property Address
. Price
. Link to the Listing
							Python Libraries
Install the following libraries via pip:

pip install requests beautifulsoup4 selenium python-dotenv

							Environment Setup
You need a .env file with:
Path to your ChromeDriver, if itâ€™s not in the default PATH.
Setup and Installation
Clone this repository:

git clone https://github.com/your-username/zillow-scraper-google-form


Navigate to the project folder:

cd zillow-scraper-google-form


Install required packages:

pip install -r requirements.txt


				Set up your Google Form and update its link in the script under form_url.
How It Works
1. Scraping Zillow Clone Data
   
The script uses requests and BeautifulSoup to scrape property listings from the Zillow Clone page:
Retrieves the links, prices, and addresses of all property listings.
Cleans and formats the data to remove unwanted characters.
2. Submitting Data to Google Form

			The Selenium library automates the submission of the scraped data:
Opens the Google Form for each property entry.

Fills in the Address, Price, and Link fields with the respective property data.
Submits the form and waits before proceeding to the next entry.
Data Flow
The data is first stored in lists (one for each data type) and is then iteratively submitted to the Google Form.
Usage
Run the script:

python main.py


Monitor Form Submission: The script will open a Chrome browser, navigate to your Google Form, and fill in each entry one by one.

Access the Google Sheet: After all entries are submitted, open the linked Google Sheet to view the data.

Expected Output

The terminal will display each successfully submitted entry.
The Google Sheet will list all scraped properties, including their addresses, prices, and listing links.
Improvements and Future Work
Potential enhancements include:
Data Validation: Add checks for empty or incorrect fields.
Error Handling: Implement retry logic for network issues.
Dynamic Delay: Use adaptive delays to ensure smooth form submissions.
Data Storage: Save scraped data locally (e.g., in CSV or database) for persistent storage.
Headless Mode: Run Selenium in headless mode for efficiency (if browser visibility is not required).

License
This project is open-source and available under the MIT License.
